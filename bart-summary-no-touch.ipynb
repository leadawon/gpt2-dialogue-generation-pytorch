{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtMHIihvhieHK8PdAINu/2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZthCIYOQAnUm"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/WARNING_PRIVATE_FOLDER/gpt2-dialogue-generation-pytorch/\n","!pip install -r requirements.txt"],"metadata":{"id":"xt3D1VjfAyV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import torch\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","from transformers import pipeline\n","summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n","\n","conversation = '''Jeff: Can I train a Transformers model on Amazon SageMaker? \n","Philipp: Sure you can use the new Hugging Face Deep Learning Container. \n","Jeff: ok.\n","Jeff: and how can I get started? \n","Jeff: where can I find documentation? \n","Philipp: ok, ok you can find everything here.                                   \n","'''\n","summarizer(conversation)[0][\"summary_text\"]"],"metadata":{"id":"KdExRag3Ay3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import *\n","from tqdm import tqdm\n","\n","\n","# For all\n","space = 'Ġ'\n","pre_quote = '’'\n","end_marks = ['.', ',', '?', '!', '...']\n","quotes = ['\"', '\\'']\n","abbreviations = ['s', 'd', 't', 'm', 're', 'll', 've', 'S', 'D', 'T', 'M', 'Re', 'Ll', 'Ve']\n","\n","# For empathetic dialogues\n","exclude_symbol = \"_conv\"\n","comma_symbol = \"_comma_\"\n","\n","# For persona chat\n","persona_chat_url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n","silence_symbol = \"__ SILENCE __\"\n","\n","\n","def load_daily():\n","    dataset = load_dataset('daily_dialog')\n","    test_dialogues = dataset['test']['dialog']\n","    \n","    return test_dialogues\n","    \n","    \n","\n","def process_token_list(token_list):\n","    token_list[0] = token_list[0].capitalize()\n","    \n","    quote_count = 0\n","    for i, token in enumerate(token_list):\n","        if space in token:\n","            if token[1:] in end_marks or token[1:] in abbreviations:\n","                token_list[i] = token[1:]\n","                \n","            if token[1:] == quotes[1]:\n","                if i<len(token_list)-1:\n","                    if token_list[i+1] in abbreviations or (token_list[i+1][0] == space and token_list[i+1][1:] in abbreviations):\n","                        token_list[i] = token[1:]\n","                        \n","        if token[0] == space and token[1:] in quotes:\n","            if quote_count % 2 == 1:\n","                token_list[i] = token[1:]\n","                quote_count = 0\n","            else:\n","                if i<len(token_list)-1 and token_list[i+1][0] == space:\n","                    token_list[i+1] = token_list[i+1][1:]\n","                quote_count += 1\n","                \n","        if token in end_marks or token[1:] in end_marks:\n","            if i<len(token_list)-1:\n","                if token_list[i+1][0] != space:\n","                    token_list[i+1] = space + token_list[i+1].capitalize()\n","                else:\n","                    token_list[i+1] = space + token_list[i+1][1:].capitalize()\n","                \n","    new_token_list = [token for token in token_list if token != space and len(token)>0]\n","    if new_token_list[-1] not in end_marks:\n","        new_token_list.append(end_marks[0])\n","        \n","    return new_token_list\n"],"metadata":{"id":"Or61Qb8bA4mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_arr = load_daily()\n","load_daily()"],"metadata":{"id":"ymhDZ8XHA7MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import difflib\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_polynomial_decay_schedule_with_warmup\n","\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from torch.nn import functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from itertools import chain\n","\n","import torch\n","import os, sys\n","import numpy as np\n","import argparse\n","import copy\n","import math\n","import random\n","\n","class Arguments:\n","    def __init__(self):\n","        self.seed = 0 \n","        self.mode=\"test\" \n","        self.data_dir=\"data\" \n","        self.model_type=\"gpt2\" \n","        self.bos_token=\"<bos>\" \n","        self.sp1_token=\"<sp1>\" \n","        self.sp2_token=\"<sp2>\" \n","        self.gpu=\"0\" \n","        self.max_len=1024 \n","        self.max_turns=5 \n","        self.top_p=0.8 \n","        self.ckpt_dir=\"saved_models\" \n","        self.ckpt_name=\"best_ckpt_epoch=3_valid_loss=2.6631\" \n","        self.end_command=\"Abort!\"\n","\n","\n","\n","#원래 shell로 들어가는 파라미터를 정의합니다.\n","\n","class Manager():\n","    def __init__(self, args, test_arr):\n","        self.args = args\n","        self.test_arr = test_arr\n","\n","        if torch.cuda.is_available():\n","            self.args.device = torch.device(f\"cuda:{self.args.gpu}\")\n","        else:\n","            self.args.device = torch.device(\"cpu\")\n","        \n","        # Tokenizer & Vocab\n","        print(\"Loading the tokenizer...\")\n","        self.tokenizer = GPT2Tokenizer.from_pretrained(self.args.model_type)\n","        special_tokens = {\n","            'bos_token': self.args.bos_token,\n","            'additional_special_tokens': [self.args.sp1_token, self.args.sp2_token]\n","        }\n","        self.args.eos_token = self.tokenizer.eos_token\n","        num_new_tokens = self.tokenizer.add_special_tokens(special_tokens)\n","        vocab = self.tokenizer.get_vocab()\n","        self.args.vocab_size = len(vocab)\n","        self.args.bos_id = vocab[self.args.bos_token]\n","        self.args.eos_id = vocab[self.args.eos_token]\n","        self.args.sp1_id = vocab[self.args.sp1_token]\n","        self.args.sp2_id = vocab[self.args.sp2_token]\n","        \n","        # Load model    \n","        print(\"Loading the model...\")\n","        self.fix_seed(self.args.seed)\n","        self.model = GPT2LMHeadModel.from_pretrained(self.args.model_type).to(self.args.device)\n","        self.model.resize_token_embeddings(self.args.vocab_size)\n","        \n","        self.args.max_len = min(self.args.max_len, self.model.config.n_ctx)\n","            \n","        \n","        \n","        if self.args.ckpt_name is not None:\n","            ckpt_path = f\"{self.args.ckpt_dir}/{self.args.ckpt_name}.ckpt\"\n","            if os.path.exists(ckpt_path):\n","                print(\"Loading the trained checkpoint...\")\n","                ckpt = torch.load(ckpt_path, map_location=self.args.device)\n","                self.model.load_state_dict(ckpt['model_state_dict'])\n","                \n","                if self.args.mode == 'train':\n","                    print(f\"The training restarts with the specified checkpoint: {self.args.ckpt_name}.ckpt.\")\n","                    self.optim.load_state_dict(ckpt['optim_state_dict'])\n","                    self.sched.load_state_dict(ckpt['sched_state_dict'])\n","                    self.best_loss = ckpt['loss']\n","                    self.last_epoch = ckpt['epoch']\n","                else:\n","                    print(\"The inference will start with the specified checkpoint.\")\n","            else:\n","                print(f\"Cannot fine the specified checkpoint {ckpt_path}.\")\n","                if self.args.mode == 'train':\n","                    print(\"Training will start with the initialized model.\")\n","                else:\n","                    print(\"Cannot inference.\")\n","                    exit()\n","              \n","        print(\"Setting finished.\")\n","        \n","    def nucleus_sampling(self, input_ids, token_type_ids, input_len):\n","        output_ids = []\n","        for pos in range(input_len, self.args.max_len):\n","            output = self.model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n","            output = F.softmax(output, dim=-1)  # (1, V)\n","            \n","            sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n","            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n","            idx_remove = cumsum_probs > self.args.top_p\n","            idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n","            idx_remove[:, 0] = False\n","            sorted_probs[idx_remove] = 0.0\n","            sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n","            \n","            probs = torch.zeros(output.shape, device=self.args.device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n","            idx = torch.multinomial(probs, 1)  # (1, 1)\n","            \n","            idx_item = idx.squeeze(-1).squeeze(-1).item()\n","            output_ids.append(idx_item)\n","            \n","            if idx_item == self.args.eos_id:\n","                break\n","                \n","            input_ids = torch.cat((input_ids, idx), dim=-1)\n","            next_type_id = torch.LongTensor([[self.args.sp2_id]]).to(self.args.device)\n","            token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n","            assert input_ids.shape == token_type_ids.shape\n","            \n","        return output_ids\n","\n","    def fix_seed(self, seed):\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        random.seed(seed)  \n","\n","    def test(self):\n","        test_arr = self.test_arr\n","        print(\"Let's start!\")\n","        self.model.eval()\n","        self.fix_seed(self.args.seed)\n","\n","        summarized_utter = []\n","        user = []\n","        ground_truth = []\n","        for utters in test_arr:\n","\n","            #summarized_utter.append(summarizer(utters[:-2])[0][\"summary_text\"]) #요약문장\n","            user.append(utters[-2]) #user utter\n","            ground_truth.append(utters[-1]) #ground truth 진짜 정답.\n","\n","        \n","        with torch.no_grad():\n","            input_hists = []\n","            ex_cnt = -1\n","            similarity = 0\n","            for utters in test_arr:\n","                #utter = input(\"You: \")\n","                ex_cnt += 1\n","                if ex_cnt == len(test_arr):\n","                    break\n","                \n","                ## summarized input ##\n","                input_hists = []\n","                if len(utters) >= 3:\n","                    string_input = ' '.join(utters[:-2])\n","                    sumrz_input = summarizer(string_input , max_length= 1 * len(string_input) // 6 , min_length=1 * len(string_input) // 12)[0][\"summary_text\"]\n","                else:\n","                    string_input = \"\"\n","                    sumrz_input = \"\"    \n","                summarized_utter.append(sumrz_input)\n","\n","                sumrz_input_ids = [self.args.sp1_id] + self.tokenizer.encode(sumrz_input) # sp1 sumar.. utter\n","                input_hists.append(sumrz_input_ids)\n","\n","                ## sumarized utter를 하나씩 꺼냅니다. ##\n","\n","                input_ids = [self.args.sp2_id] + self.tokenizer.encode(utters[-2]) # sp2 user utter\n","                input_hists.append(input_ids)\n","                \n","                #if len(input_hists) >= self.args.max_turns:\n","                    #num_exceeded = len(input_hists) - self.args.max_turns + 1\n","                    #input_hists = input_hists[num_exceeded:]\n","                # 역시 턴 개념은 사용하지 않습니다.\n","                    \n","                input_ids = [self.args.bos_id] + list(chain.from_iterable(input_hists)) + [self.args.sp1_id] # 2 -> 1\n","                #start_sp_id = input_hists[0][0]\n","                start_sp_id = self.args.sp1_id\n","                \n","\n","\n","                #next_sp_id = self.args.sp1_id if start_sp_id == self.args.sp2_id else self.args.sp2_id\n","                next_sp_id = self.args.sp2_id\n","\n","                assert start_sp_id != next_sp_id\n","                token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)] \n","                assert len(token_type_ids) == len(input_hists)\n","                token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [self.args.sp1_id] # 2 -> 1\n","                assert len(input_ids) == len(token_type_ids)\n","                input_len = len(input_ids)\n","                \n","                input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(self.args.device)\n","                token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(self.args.device)\n","                \n","                output_ids = self.nucleus_sampling(input_ids, token_type_ids, input_len)\n","           \n","                # output_ids = self.model.generate(\n","                #     input_ids=input_ids, token_type_ids=token_type_ids, pad_token_id=self.args.eos_id,\n","                #     do_sample=True, top_p=self.args.top_p, max_length=self.args.max_len,\n","                #     output_hidden_states=True, output_scores=True, return_dict_in_generate=True,\n","                # ).sequences\n","                # output_ids = output_ids[0].tolist()[input_len:]\n","                res = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n","\n","                similarity += compute_similarity(res, ground_truth[ex_cnt])\n","\n","                print(f\"summarized : {summarized_utter[ex_cnt]}\")\n","                print(f\"user : {user[ex_cnt]}\")\n","                print(f\"res : {res}\\n gt : {ground_truth[ex_cnt]}\")\n","\n","                # 예측한 문장과 ground truth를 비교할 수 있습니다.\n","                # 아직 눈으로 밖에 비교할 방법이 없음.\n","\n","                #print(f\"Bot: {res}\")\n","                #input_hists.append([self.args.sp2_id] + self.tokenizer.encode(res))\n","        print(f\"문자열 간에 유사도 : {similarity / len(test_arr)}\")\n","\n","\n","def compute_similarity(string1, string2):\n","\n","    matcher = difflib.SequenceMatcher(None, string1, string2)\n","    return matcher.ratio()\n","\n","# 두 문자열을 비교하는 함수이나...\n","# 큰 의미는 없는것 같다.\n","\n","      "],"metadata":{"id":"Op8_1mrvA8lp"},"execution_count":null,"outputs":[]}]}